Starting job 190612 on gpu-1-[1-2] on Olivia at Wed Feb 25 10:24:25 CET 2026

ROOT_DIR = /cluster/home/marcink/hpe_cug_paper/openmpi_on_ss11
Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None

Lmod is automatically replacing "cce/19.0.0" with "gcc-native/14.2".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-libsci/25.03.0     2) cray-mpich/8.1.32


The following have been reloaded with a version change:
  1) gcc-native/14.2 => gcc-native/13.2


Currently Loaded Modules:
  1) init-NRIS                      (S)   9) craype/2.7.34
  2) craype-arm-grace                    10) cray-dsmml/0.3.1
  3) libfabric/1.22.0                    11) PrgEnv-gnu/8.6.0
  4) craype-network-ofi                  12) gcc-native/13.2
  5) perftools-base/25.03.0              13) cray-mpich/8.1.32
  6) xpmem/2.11.3-1.3_gdbda01a1eb3d      14) cray-libsci/25.03.0
  7) CrayEnv                             15) craype-accel-nvidia90
  8) cuda/12.6

  Where:
   S:  Module is Sticky, requires --force to unload or purge

 

-- srun osu_bibw -W 64 D D
cpu-bind=MASK - gpu-1-1, task  0  0 [3085788]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-2, task  1  0 [389608]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 0 gpu 0 nic cxi0

# OSU MPI-CUDA Bi-Directional Bandwidth Test v7.5.2
# Datatype: MPI_CHAR.
# Size      Bandwidth (MB/s)
1                       1.87
2                       3.78
4                       7.57
8                      15.12
16                     30.21
32                     60.29
64                    120.73
128                   238.60
256                   486.48
512                   976.68
1024                 1944.41
2048                 3902.20
4096                 7816.73
8192                15613.58
16384               25755.33
32768               29975.44
65536               38024.09
131072              42040.03
262144              44105.88
524288              45260.28
1048576             45742.44
2097152             45885.69
4194304             46090.98
-- srun osu_bibw -W 1 D D
cpu-bind=MASK - gpu-1-2, task  1  0 [390208]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  0  0 [3086401]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 0 gpu 0 nic cxi0

# OSU MPI-CUDA Bi-Directional Bandwidth Test v7.5.2
# Datatype: MPI_CHAR.
# Size      Bandwidth (MB/s)
1                       0.54
2                       1.07
4                       2.14
8                       4.29
16                      8.17
32                     17.07
64                     34.72
128                    67.58
256                   107.92
512                   218.05
1024                  420.57
2048                  829.80
4096                 1579.32
8192                 3044.10
16384                5479.05
32768                6580.15
65536               11341.94
131072              17631.07
262144              25249.73
524288              32588.31
1048576             38230.37
2097152             42080.52
4194304             44256.85
-- srun osu_latency D D
cpu-bind=MASK - gpu-1-2, task  1  0 [390991]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  0  0 [3087198]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 0 gpu 0 nic cxi0

# OSU MPI-CUDA Latency Test v7.5.2
# Datatype: MPI_CHAR.
# Size       Avg Latency(us)
1                       2.65
2                       2.63
4                       2.80
8                       2.76
16                      2.75
32                      2.67
64                      2.54
128                     3.23
256                     3.57
512                     3.61
1024                    3.69
2048                    3.87
4096                    4.05
8192                    4.39
16384                   4.97
32768                   8.47
65536                   9.83
131072                 12.55
262144                 18.05
524288                 29.05
1048576                50.61
2097152                93.89
4194304               180.19
-- srun osu_bibw -W 64 H H
cpu-bind=MASK - gpu-1-2, task  1  0 [391590]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  0  0 [3087812]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 0 gpu 0 nic cxi0

# OSU MPI Bi-Directional Bandwidth Test v7.5.2
# Datatype: MPI_CHAR.
# Size      Bandwidth (MB/s)
1                       1.61
2                       3.23
4                       6.49
8                      12.93
16                     25.63
32                     51.55
64                    103.47
128                   205.44
256                   414.84
512                   828.01
1024                 1652.80
2048                 3322.85
4096                 6647.02
8192                13304.24
16384               24707.18
32768               31110.57
65536               37436.29
131072              41521.66
262144              43971.60
524288              45304.96
1048576             45993.61
2097152             46349.49
4194304             46532.19
-- srun osu_bibw -W 1 H H
cpu-bind=MASK - gpu-1-2, task  1  0 [392189]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0
cpu-bind=MASK - gpu-1-1, task  0  0 [3088427]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0

# OSU MPI Bi-Directional Bandwidth Test v7.5.2
# Datatype: MPI_CHAR.
# Size      Bandwidth (MB/s)
1                       0.52
2                       1.05
4                       2.10
8                       4.09
16                      8.38
32                     16.80
64                     34.04
128                    64.67
256                   111.73
512                   221.69
1024                  429.63
2048                  848.05
4096                 1623.08
8192                 3089.29
16384                5530.84
32768                6758.20
65536               11222.30
131072              18395.17
262144              26373.56
524288              33657.72
1048576             39106.34
2097152             42440.21
4194304             44513.47
-- srun osu_latency H H
cpu-bind=MASK - gpu-1-1, task  0  0 [3089265]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0
cpu-bind=MASK - gpu-1-2, task  1  0 [393017]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0

# OSU MPI Latency Test v7.5.2
# Datatype: MPI_CHAR.
# Size       Avg Latency(us)
1                       2.59
2                       2.59
4                       2.69
8                       2.70
16                      2.68
32                      2.62
64                      2.50
128                     3.20
256                     3.36
512                     3.37
1024                    3.46
2048                    3.60
4096                    3.87
8192                    4.15
16384                   4.66
32768                   8.02
65536                   9.37
131072                 12.20
262144                 17.57
524288                 28.45
1048576                50.23
2097152                93.77
4194304               180.19
---------------- 8.1.32 ---------------
-- srun osu_xccl_bibw -W 64 D D
cpu-bind=MASK - gpu-1-1, task  0  0 [3089881]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-2, task  1  0 [393616]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0
#Using NCCL
using SLURM envars
rank local 0 gpu 0 nic cxi0
#Using NCCL
# Send Buffer on DEVICE (D) and Receive Buffer on DEVICE (D)
# Size      Bandwidth (MB/s)
1                       0.13
2                       0.27
4                       0.53
8                       1.07
16                      2.18
32                      4.35
64                      8.72
128                    16.57
256                    33.53
512                    67.05
1024                  133.27
2048                  263.62
4096                  521.07
8192                 1016.05
16384                1943.44
32768                2894.32
65536                4780.03
131072               9535.93
262144              15398.61
524288              22885.16
1048576             32491.78
2097152             37462.51
4194304             41328.02
-- srun osu_xccl_bibw -W 1 D D
cpu-bind=MASK - gpu-1-2, task  1  0 [394405]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  0  0 [3090683]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 0 gpu 0 nic cxi0
#Using NCCL
#Using NCCL
# Send Buffer on DEVICE (D) and Receive Buffer on DEVICE (D)
# Size      Bandwidth (MB/s)
1                       0.07
2                       0.15
4                       0.30
8                       0.59
16                      1.20
32                      1.89
64                      4.82
128                     9.25
256                    18.31
512                    36.53
1024                   73.18
2048                  147.80
4096                  294.00
8192                  462.77
16384                1132.59
32768                1901.91
65536                3333.59
131072               6298.85
262144              10866.19
524288              16744.58
1048576             24545.26
2097152             29707.77
4194304             36509.81
-- srun osu_xccl_latency D D
cpu-bind=MASK - gpu-1-1, task  0  0 [3091302]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-2, task  1  0 [395010]: mask 0xffffffffffffffffff set
using SLURM envars
rank local 0 gpu 0 nic cxi0
#Using NCCL
using SLURM envars
rank local 0 gpu 0 nic cxi0
#Using NCCL
# Send Buffer on DEVICE (D) and Receive Buffer on DEVICE (D)
# Size          Latency (us)
1                      26.63
2                      26.59
4                      26.48
8                      26.57
16                     26.37
32                     26.40
64                     26.42
128                    27.23
256                    27.15
512                    27.29
1024                   27.31
2048                   27.50
4096                   27.57
8192                   27.87
16384                  28.68
32768                  33.81
65536                  38.41
131072                 39.24
262144                 44.99
524288                 55.67
1048576                79.79
2097152               122.86
4194304               209.44

Job 190612 consumed 0.0 billing hours and 0.0 GPU hours from project nn9997k.

Submitted 2026-02-25T10:24:24; waited 0.0 seconds in the queue after becoming eligible to run.

Requested wallclock time: 10.0 minutes
Elapsed wallclock time:   29.0 seconds

Job exited normally.

Task and CPU statistics:
ID                 CPUs  Tasks  CPU util                Start  Elapsed  Exit status
190612              144            0.0 %  2026-02-25T10:24:24   29.0 s  0
190612.batch         72      1     0.1 %  2026-02-25T10:24:24   29.0 s  0
190612.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:26    3.0 s  0
190612.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:29    1.0 s  0
190612.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:30    2.0 s  0
190612.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:32    2.0 s  0
190612.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:34    1.0 s  0
190612.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:35    2.0 s  0
190612.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:37    4.0 s  0
190612.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:41    1.0 s  0
190612.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:42   11.0 s  0

Used CPU time:   1.1 CPU seconds
Unused CPU time: 1.2 CPU hours

Memory statistics, in GiB:
ID                  Alloc   Usage
190612             1400.0        
190612.batch        700.0     0.2
190612.gpubind.sh  1400.0     0.6
190612.gpubind.sh  1400.0     0.4
190612.gpubind.sh  1400.0     0.4
190612.gpubind.sh  1400.0     0.4
190612.gpubind.sh  1400.0     0.4
190612.gpubind.sh  1400.0     0.4
190612.gpubind.sh  1400.0     1.5
190612.gpubind.sh  1400.0     1.4
190612.gpubind.sh  1400.0     1.4

Job 190612 completed at Wed Feb 25 10:24:53 CET 2026
