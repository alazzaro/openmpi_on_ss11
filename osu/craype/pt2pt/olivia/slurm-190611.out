Starting job 190611 on gpu-1-1 on Olivia at Wed Feb 25 10:23:48 CET 2026

ROOT_DIR = /cluster/home/marcink/hpe_cug_paper/openmpi_on_ss11
Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None

Lmod is automatically replacing "cce/19.0.0" with "gcc-native/14.2".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-libsci/25.03.0     2) cray-mpich/8.1.32


The following have been reloaded with a version change:
  1) gcc-native/14.2 => gcc-native/13.2


Currently Loaded Modules:
  1) init-NRIS                      (S)   9) craype/2.7.34
  2) craype-arm-grace                    10) cray-dsmml/0.3.1
  3) libfabric/1.22.0                    11) PrgEnv-gnu/8.6.0
  4) craype-network-ofi                  12) gcc-native/13.2
  5) perftools-base/25.03.0              13) cray-mpich/8.1.32
  6) xpmem/2.11.3-1.3_gdbda01a1eb3d      14) cray-libsci/25.03.0
  7) CrayEnv                             15) craype-accel-nvidia90
  8) cuda/12.6

  Where:
   S:  Module is Sticky, requires --force to unload or purge

 

-- srun osu_bibw -W 64 D D
cpu-bind=MASK - gpu-1-1, task  0  0 [3076799]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  1  1 [3076805]: mask 0xffffffffffffffffff000000000000000000 set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 1 gpu 1 nic cxi1

# OSU MPI-CUDA Bi-Directional Bandwidth Test v7.5.2
# Datatype: MPI_CHAR.
# Size      Bandwidth (MB/s)
1                       0.14
2                       0.28
4                       0.56
8                       1.12
16                      2.24
32                      4.46
64                      8.91
128                    17.81
256                    35.39
512                    70.19
1024                  411.60
2048                  847.42
4096                 1674.94
8192                 3370.78
16384                6777.28
32768               13705.79
65536               27237.27
131072              53500.12
262144             102693.97
524288             154618.80
1048576            191297.33
2097152            216228.12
4194304            231730.98
-- srun osu_bibw -W 1 D D
cpu-bind=MASK - gpu-1-1, task  0  0 [3077443]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  1  1 [3077444]: mask 0xffffffffffffffffff000000000000000000 set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 1 gpu 1 nic cxi1

# OSU MPI-CUDA Bi-Directional Bandwidth Test v7.5.2
# Datatype: MPI_CHAR.
# Size      Bandwidth (MB/s)
1                       0.13
2                       0.25
4                       0.50
8                       1.00
16                      1.91
32                      3.97
64                      7.79
128                    15.73
256                    31.10
512                    62.83
1024                  163.11
2048                  324.65
4096                  645.23
8192                 1289.30
16384                2556.26
32768                5109.32
65536                9835.88
131072              18822.04
262144              34964.89
524288              60320.71
1048576             96422.33
2097152            137447.41
4194304            177923.97
-- srun osu_latency D D
cpu-bind=MASK - gpu-1-1, task  0  0 [3078063]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  1  1 [3078064]: mask 0xffffffffffffffffff000000000000000000 set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 1 gpu 1 nic cxi1

# OSU MPI-CUDA Latency Test v7.5.2
# Datatype: MPI_CHAR.
# Size       Avg Latency(us)
1                      16.44
2                      16.40
4                      16.40
8                      16.38
16                     16.41
32                     16.46
64                     16.52
128                    16.56
256                    16.64
512                    16.76
1024                   11.81
2048                   11.88
4096                   11.95
8192                   12.08
16384                  12.10
32768                  12.15
65536                  12.53
131072                 13.14
262144                 14.18
524288                 16.01
1048576                20.17
2097152                28.39
4194304                44.62
-- srun osu_bibw -W 64 H H
cpu-bind=MASK - gpu-1-1, task  0  0 [3079055]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  1  1 [3079056]: mask 0xffffffffffffffffff000000000000000000 set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 1 gpu 1 nic cxi1

# OSU MPI Bi-Directional Bandwidth Test v7.5.2
# Datatype: MPI_CHAR.
# Size      Bandwidth (MB/s)
1                       0.56
2                       1.13
4                       2.26
8                       4.52
16                      9.07
32                     18.12
64                     35.78
128                    71.90
256                   143.54
512                   281.60
1024                  412.38
2048                  827.39
4096                 1627.94
8192                 3204.92
16384                6185.31
32768               11569.86
65536               19793.52
131072              31732.31
262144              42704.49
524288              54580.16
1048576             61114.67
2097152             58841.20
4194304             56950.30
-- srun osu_bibw -W 1 H H
cpu-bind=MASK - gpu-1-1, task  0  0 [3079670]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  1  1 [3079671]: mask 0xffffffffffffffffff000000000000000000 set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 1 gpu 1 nic cxi1

# OSU MPI Bi-Directional Bandwidth Test v7.5.2
# Datatype: MPI_CHAR.
# Size      Bandwidth (MB/s)
1                       0.31
2                       0.88
4                       1.77
8                       3.29
16                      6.47
32                     12.99
64                     26.30
128                    49.82
256                   111.22
512                   209.20
1024                  237.54
2048                  477.02
4096                  933.32
8192                 1869.43
16384                3686.91
32768                7046.50
65536               13644.73
131072              22937.94
262144              31654.93
524288              41949.92
1048576             42393.51
2097152             46209.90
4194304             38336.59
-- srun osu_latency H H
cpu-bind=MASK - gpu-1-1, task  0  0 [3080285]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  1  1 [3080286]: mask 0xffffffffffffffffff000000000000000000 set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 1 gpu 1 nic cxi1

# OSU MPI Latency Test v7.5.2
# Datatype: MPI_CHAR.
# Size       Avg Latency(us)
1                       3.84
2                       3.84
4                       3.84
8                       3.85
16                      3.85
32                      3.96
64                      4.02
128                     4.13
256                     4.13
512                     4.13
1024                    6.30
2048                    6.35
4096                    6.39
8192                    6.48
16384                   6.74
32768                   7.07
65536                   7.89
131072                  9.65
262144                 13.31
524288                 22.23
1048576                36.21
2097152                75.24
4194304               169.18
---------------- 8.1.32 ---------------
-- srun osu_xccl_bibw -W 64 D D
cpu-bind=MASK - gpu-1-1, task  0  0 [3081376]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  1  1 [3081382]: mask 0xffffffffffffffffff000000000000000000 set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 1 gpu 1 nic cxi1
#Using NCCL
#Using NCCL
# Send Buffer on DEVICE (D) and Receive Buffer on DEVICE (D)
# Size      Bandwidth (MB/s)
1                       0.45
2                       0.88
4                       1.79
8                       3.58
16                      7.18
32                     14.35
64                     28.56
128                    57.18
256                   115.68
512                   228.82
1024                  454.79
2048                  894.51
4096                 1646.17
8192                 2794.47
16384                4576.19
32768                7287.32
65536               12526.65
131072              24531.26
262144              45882.63
524288              68035.54
1048576             92373.97
2097152            114478.51
4194304            139178.23
-- srun osu_xccl_bibw -W 1 D D
cpu-bind=MASK - gpu-1-1, task  0  0 [3082058]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  1  1 [3082059]: mask 0xffffffffffffffffff000000000000000000 set
using SLURM envars
rank local 0 gpu 0 nic cxi0
#Using NCCL
using SLURM envars
rank local 1 gpu 1 nic cxi1
#Using NCCL
# Send Buffer on DEVICE (D) and Receive Buffer on DEVICE (D)
# Size      Bandwidth (MB/s)
1                       0.15
2                       0.29
4                       0.58
8                       1.16
16                      2.33
32                      4.66
64                      9.40
128                    18.86
256                    38.24
512                    75.66
1024                  153.48
2048                  311.22
4096                  580.45
8192                 1114.68
16384                1982.26
32768                3452.00
65536                6180.99
131072               4691.90
262144              23894.70
524288              41960.66
1048576             65244.64
2097152             90260.75
4194304            116484.10
-- srun osu_xccl_latency D D
cpu-bind=MASK - gpu-1-1, task  0  0 [3082879]: mask 0xffffffffffffffffff set
cpu-bind=MASK - gpu-1-1, task  1  1 [3082880]: mask 0xffffffffffffffffff000000000000000000 set
using SLURM envars
rank local 0 gpu 0 nic cxi0
using SLURM envars
rank local 1 gpu 1 nic cxi1
#Using NCCL
#Using NCCL
# Send Buffer on DEVICE (D) and Receive Buffer on DEVICE (D)
# Size          Latency (us)
1                      12.98
2                      12.90
4                      12.87
8                      12.78
16                     12.78
32                     12.73
64                     12.70
128                    12.71
256                    12.65
512                    12.66
1024                   12.70
2048                   12.79
4096                   12.91
8192                   13.56
16384                  14.42
32768                  17.79
65536                  19.67
131072                 19.95
262144                 20.55
524288                 24.13
1048576                31.08
2097152                44.77
4194304                62.49

Job 190611 consumed 0.0 billing hours and 0.0 GPU hours from project nn9997k.

Submitted 2026-02-25T10:23:47; waited 0.0 seconds in the queue after becoming eligible to run.

Requested wallclock time: 10.0 minutes
Elapsed wallclock time:   29.0 seconds

Job exited normally.

Task and CPU statistics:
ID                 CPUs  Tasks  CPU util                Start  Elapsed  Exit status
190611              144            0.0 %  2026-02-25T10:23:47   29.0 s  0
190611.batch        144      1     0.0 %  2026-02-25T10:23:47   29.0 s  0
190611.gpubind.sh   144      2     0.0 %  2026-02-25T10:23:49    4.0 s  0
190611.gpubind.sh   144      2     0.0 %  2026-02-25T10:23:53    1.0 s  0
190611.gpubind.sh   144      2     0.0 %  2026-02-25T10:23:54    6.0 s  0
190611.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:00    1.0 s  0
190611.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:01    1.0 s  0
190611.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:02    3.0 s  0
190611.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:05    3.0 s  0
190611.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:08    2.0 s  0
190611.gpubind.sh   144      2     0.0 %  2026-02-25T10:24:10    6.0 s  0

Used CPU time:   1.1 CPU seconds
Unused CPU time: 1.2 CPU hours

Memory statistics, in GiB:
ID                  Alloc   Usage
190611              700.0        
190611.batch        700.0     0.2
190611.gpubind.sh   700.0     0.3
190611.gpubind.sh   700.0     0.2
190611.gpubind.sh   700.0     0.2
190611.gpubind.sh   700.0     0.2
190611.gpubind.sh   700.0     0.2
190611.gpubind.sh   700.0     0.2
190611.gpubind.sh   700.0     1.0
190611.gpubind.sh   700.0     0.9
190611.gpubind.sh   700.0     0.9

Job 190611 completed at Wed Feb 25 10:24:17 CET 2026
